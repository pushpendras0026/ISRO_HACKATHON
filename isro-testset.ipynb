{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29047,"sourceType":"datasetVersion","datasetId":22655}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch albumentations --quiet\n\nimport os\nimport torch\nimport numpy as np\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = smp.Unet(\n    encoder_name=\"resnet34\",        # could also try \"efficientnet-b0\"\n    encoder_weights=\"imagenet\",     # use pretrained weights\n    in_channels=3,                  # RGB input\n    classes=12                      # number of segmentation classes\n).cuda()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ce_loss = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ARBTLoss(nn.Module):\n    def __init__(self, class_weights=None, delta=0.7, gamma=1.5, lam=0.2, epsilon=1e-6):\n        super().__init__()\n        self.class_weights = class_weights\n        self.delta = delta  # Controls balance between FN and FP\n        self.gamma = gamma  # Focal power\n        self.lam = lam      # Focal weighting strength\n        self.epsilon = epsilon\n\n    def forward(self, logits, targets):\n        num_classes = logits.shape[1]\n        probs = F.softmax(logits, dim=1)\n        one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n\n        total_loss = 0.0\n        for c in range(num_classes):\n            p = probs[:, c]\n            g = one_hot[:, c]\n\n            tp = (p * g).sum(dim=(1,2))\n            fp = (p * (1 - g)).sum(dim=(1,2))\n            fn = ((1 - p) * g).sum(dim=(1,2))\n\n            tversky = (tp + self.epsilon) / (tp + self.delta * fp + (1 - self.delta) * fn + self.epsilon)\n            tversky_loss = 1 - tversky\n\n            focal_term = torch.mean(torch.abs(p - g) ** self.gamma, dim=(1,2))\n            loss = tversky_loss + self.lam * focal_term\n\n            if self.class_weights is not None:\n                loss = loss * self.class_weights[c]\n\n            total_loss += loss.mean()\n\n        return total_loss / num_classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CASM:\n    def __init__(self, base_optimizer, model, rho=0.05, kappa=1.0, epsilon=1e-12):\n        self.model = model\n        self.rho = rho\n        self.kappa = kappa\n        self.epsilon = epsilon\n        self.base_optimizer = base_optimizer\n\n    def step(self, closure, rare_ratio=0.0):\n        adaptive_rho = self.rho * (1 + self.kappa * (1 - rare_ratio))\n\n        grads = []\n        for group in self.base_optimizer.param_groups:\n            for p in group['params']:\n                if p.grad is None: continue\n                grads.append(p.grad.clone())\n                e_w = adaptive_rho * p.grad / (p.grad.norm() + self.epsilon)\n                p.add_(e_w)\n\n        loss = closure()  # tracked by autograd\n        loss.backward()\n\n        self.base_optimizer.step()\n\n        # Optionally restore grads\n        for group, grad_group in zip(self.base_optimizer.param_groups, grads):\n            for p, g in zip(group['params'], grad_group):\n                if p.grad is None: continue\n                p.grad.copy_(g)\n\n        return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloader, loss_fn, optimizer, epochs=15, use_casm=False):\n    model.train()\n    history = {\"loss\": []}\n\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for images, masks in tqdm(dataloader):\n            images, masks = images.cuda(), masks.cuda()\n\n            def closure():\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = loss_fn(outputs, masks)\n                return loss\n\n            outputs = model(images)\n            loss = loss_fn(outputs, masks)\n\n            if use_casm:\n                rare_ratio = (masks == 5).sum() / (masks.numel())  # adjust class index as needed\n            \n                optimizer.base_optimizer.zero_grad()  # FIXED: call on base_optimizer\n                optimizer.step(\n                    closure=lambda: loss_fn(model(images), masks), \n                    rare_ratio=rare_ratio.item()\n                )\n            else:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader)\n        history[\"loss\"].append(avg_loss)\n        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n\n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, num_classes=12):\n    model.eval()\n    preds_all = []\n    masks_all = []\n\n    with torch.no_grad():\n        for images, masks in tqdm(dataloader):\n            images = images.cuda()\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            masks = masks.numpy()\n\n            preds_all.extend(preds.reshape(-1))\n            masks_all.extend(masks.reshape(-1))\n\n    preds_all = np.array(preds_all)\n    masks_all = np.array(masks_all)\n\n    cm = confusion_matrix(masks_all, preds_all, labels=list(range(num_classes)))\n    iou = np.diag(cm) / (cm.sum(1) + cm.sum(0) - np.diag(cm) + 1e-6)\n    f1 = 2 * np.diag(cm) / (cm.sum(1) + cm.sum(0) + 1e-6)\n\n    return iou, f1, cm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_loss(history, label):\n    plt.plot(history[\"loss\"], label=label)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_prediction(model, dataset, idx=0):\n    model.eval()\n    image, mask = dataset[idx]\n    with torch.no_grad():\n        output = model(image.unsqueeze(0).cuda())\n        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].imshow(image.permute(1,2,0).cpu())\n    axs[0].set_title(\"Input Image\")\n    axs[1].imshow(mask)\n    axs[1].set_title(\"Ground Truth\")\n    axs[2].imshow(pred)\n    axs[2].set_title(\"Prediction\")\n    for ax in axs: ax.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import optim\n\ndef run_all_configs(train_loader, val_loader):\n    configs = {\n        \"CASM + ARBT\": {\"loss\": ARBTLoss(), \"casm\": True},\n        \"Adam + CE\": {\"loss\": nn.CrossEntropyLoss(), \"casm\": False},\n        \"Adam + ARBT\": {\"loss\": ARBTLoss(), \"casm\": False}\n    }\n\n    results = {}\n    for name, cfg in configs.items():\n        print(f\"\\nðŸš€ Running: {name}\")\n        model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=12).cuda()\n\n        base_optimizer = optim.Adam(model.parameters(), lr=1e-3)\n        optimizer = CASM(base_optimizer, model) if cfg[\"casm\"] else base_optimizer\n\n        history = train_model(model, train_loader, cfg[\"loss\"], optimizer, epochs=15, use_casm=cfg[\"casm\"])\n        iou, f1, cm = evaluate(model, val_loader)\n\n        results[name] = {\n            \"history\": history,\n            \"iou\": iou,\n            \"f1\": f1,\n            \"cm\": cm,\n            \"model\": model\n        }\n\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CITYSCAPES_COLORS = {\n    (128, 64,128): 0,  # road\n    (244, 35,232): 1,  # sidewalk\n    (70, 70, 70): 2,   # building\n    (102,102,156): 3,  # wall\n    (190,153,153): 4,  # fence\n    (153,153,153): 5,  # pole\n    (250,170, 30): 6,  # traffic light\n    (220,220,  0): 7,  # traffic sign\n    (107,142, 35): 8,  # vegetation\n    (152,251,152): 9,  # terrain\n    (70,130,180):10,   # sky\n    (220, 20, 60):11,  # person\n}\n\ndef rgb_to_class(mask_rgb):\n        h, w, _ = mask_rgb.shape\n        mask = np.zeros((h, w), dtype=np.uint8)\n    \n        for rgb, cls in CITYSCAPES_COLORS.items():\n            matches = np.all(mask_rgb == rgb, axis=-1)\n            mask[matches] = cls\n    \n        return mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms as T\nfrom torchvision.transforms import functional as TF\n\nclass CityscapesImagePairsDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.image_paths = sorted([\n            os.path.join(root_dir, fname)\n            for fname in os.listdir(root_dir)\n            if fname.endswith('.jpg')\n        ])\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        full_image = Image.open(img_path).convert(\"RGB\")\n        w, h = full_image.size\n        mid = w // 2\n\n        image = full_image.crop((0, 0, mid, h))  # Left side: RGB\n        mask = full_image.crop((mid, 0, w, h))   # Right side: Mask\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        if self.transform:\n            mask = rgb_to_class(mask)\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n\n        return image, mask.long()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ncommon_transform = A.Compose([\n    A.Resize(256, 512),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2()\n])\n\ntrain_dataset = CityscapesImagePairsDataset(\n    root_dir='/kaggle/input/cityscapes-image-pairs/cityscapes_data/train',\n    transform=common_transform\n)\n\nval_dataset = CityscapesImagePairsDataset(\n    root_dir='/kaggle/input/cityscapes-image-pairs/cityscapes_data/val',\n    transform=common_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = run_all_configs(train_loader, val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, r in results.items():\n    plot_loss(r[\"history\"], label=name)\nplt.title(\"Training Loss Comparison\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metric(results, metric='iou'):\n    plt.figure(figsize=(10,5))\n    for name in results:\n        values = results[name][metric]\n        plt.plot(values, label=name)\n    plt.title(f\"Per-Class {metric.upper()} Comparison\")\n    plt.xlabel(\"Class Index\")\n    plt.ylabel(metric.upper())\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name in results:\n    print(f\"Example Prediction from: {name}\")\n    visualize_prediction(results[name][\"model\"], train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nfor name, r in results.items():\n    plt.plot(r[\"iou\"], label=name)\nplt.title(\"Per-Class IoU Comparison\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"IoU\")\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nfor name, r in results.items():\n    plt.plot(r[\"f1\"], label=name)\nplt.title(\"Per-Class F1 Score Comparison\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"F1 Score\")\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rare_class_index = 5  # change based on your definition\n\nfor name, r in results.items():\n    print(f\"{name}: Rare-Class-{rare_class_index} IoU = {r['iou'][rare_class_index]:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\ncm = results[\"CASM + ARBT\"][\"cm\"]\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Confusion Matrix - CASM + ARBT\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nsummary_data = []\nfor name, r in results.items():\n    mean_iou = np.mean(r[\"iou\"])\n    mean_f1 = np.mean(r[\"f1\"])\n    rare_iou = r[\"iou\"][rare_class_index]\n    rare_f1 = r[\"f1\"][rare_class_index]\n    summary_data.append([name, mean_iou, mean_f1, rare_iou, rare_f1])\n\ndf = pd.DataFrame(summary_data, columns=[\"Model\", \"Mean IoU\", \"Mean F1\", f\"Rare IoU (Class {rare_class_index})\", f\"Rare F1 (Class {rare_class_index})\"])\ndisplay(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"optimizer_comparison_summary.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}